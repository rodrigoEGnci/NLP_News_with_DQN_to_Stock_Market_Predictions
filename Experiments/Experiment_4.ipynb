{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7509580-31bd-44fa-b2c1-151cc280572c",
   "metadata": {},
   "source": [
    "# Experiment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78e830e5-884f-4ae2-8d23-9d0430db2a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prob with short periods of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57b3176b-bde3-404f-a5ce-fc3e58e82bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9898cb0-5469-43e4-88cf-f78e6fb5022b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999-07-26</td>\n",
       "      <td>85.789774</td>\n",
       "      <td>84.844432</td>\n",
       "      <td>84.923210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999-07-27</td>\n",
       "      <td>86.469216</td>\n",
       "      <td>85.317081</td>\n",
       "      <td>85.632195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999-07-28</td>\n",
       "      <td>86.538133</td>\n",
       "      <td>85.454929</td>\n",
       "      <td>85.907906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999-07-29</td>\n",
       "      <td>85.238351</td>\n",
       "      <td>84.017284</td>\n",
       "      <td>84.706596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999-07-30</td>\n",
       "      <td>85.297373</td>\n",
       "      <td>83.544552</td>\n",
       "      <td>83.662720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       High        Low      Close\n",
       "0  1999-07-26  85.789774  84.844432  84.923210\n",
       "1  1999-07-27  86.469216  85.317081  85.632195\n",
       "2  1999-07-28  86.538133  85.454929  85.907906\n",
       "3  1999-07-29  85.238351  84.017284  84.706596\n",
       "4  1999-07-30  85.297373  83.544552  83.662720"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read DataFrame\n",
    "SPY_stocks = pd.read_csv(\"../data/Preprocessed/SPY_Data.csv\")\n",
    "SPY_stocks = SPY_stocks.drop(['Unnamed: 0'], axis=1)\n",
    "SPY_stocks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1910d6e-44e6-49fc-b3ee-3739df1c9e95",
   "metadata": {},
   "source": [
    "### Generate Input Parameters of NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0509416-752c-4679-96e4-fb11e7e4cd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_SPY = SPY_stocks.copy()\n",
    "\n",
    "# --- Simple Moving Averages ---\n",
    "inputs_SPY['SMA_5'] = inputs_SPY['Close'].rolling(window=5).mean()\n",
    "inputs_SPY['SMA_20'] = inputs_SPY['Close'].rolling(window=20).mean()\n",
    "inputs_SPY['SMA_50'] = inputs_SPY['Close'].rolling(window=50).mean()\n",
    "\n",
    "# --- RSI (14) ---\n",
    "delta = inputs_SPY['Close'].diff()\n",
    "gain = delta.clip(lower=0)\n",
    "loss = -delta.clip(upper=0)\n",
    "avg_gain = gain.rolling(window=14).mean()\n",
    "avg_loss = loss.rolling(window=14).mean()\n",
    "rs = avg_gain / (avg_loss + 1e-10)\n",
    "inputs_SPY['RSI_14'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# --- MACD and Signal ---\n",
    "ema_12 = inputs_SPY['Close'].ewm(span=12, adjust=False).mean()\n",
    "ema_26 = inputs_SPY['Close'].ewm(span=26, adjust=False).mean()\n",
    "inputs_SPY['MACD'] = ema_12 - ema_26\n",
    "inputs_SPY['MACD_Signal'] = inputs_SPY['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# --- Bollinger Bands (%B) ---\n",
    "sma_20 = inputs_SPY['Close'].rolling(window=20).mean()\n",
    "std_20 = inputs_SPY['Close'].rolling(window=20).std()\n",
    "upper_band = sma_20 + 2 * std_20\n",
    "lower_band = sma_20 - 2 * std_20\n",
    "inputs_SPY['Bollinger_b'] = (inputs_SPY['Close'] - lower_band) / (upper_band - lower_band + 1e-10)\n",
    "\n",
    "# --- ATR (14) ---\n",
    "high_low = inputs_SPY['High'] - inputs_SPY['Low']\n",
    "high_close = np.abs(inputs_SPY['High'] - inputs_SPY['Close'].shift())\n",
    "low_close = np.abs(inputs_SPY['Low'] - inputs_SPY['Close'].shift())\n",
    "true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "inputs_SPY['ATR_14'] = true_range.rolling(window=14).mean()\n",
    "\n",
    "# --- Momentum (10) ---\n",
    "inputs_SPY['Momentum_10'] = inputs_SPY['Close'] - inputs_SPY['Close'].shift(10)\n",
    "\n",
    "# ---- Parameters of Experiment 3 -----\n",
    "inputs_SPY['Relative_price'] = inputs_SPY['Close']/inputs_SPY['SMA_20']\n",
    "inputs_SPY['Norm_Volatily'] = inputs_SPY['ATR_14']/inputs_SPY['Close']\n",
    "inputs_SPY['Scal_RSI'] = inputs_SPY['RSI_14']/100\n",
    "\n",
    "inputs_SPY = inputs_SPY.drop(['Low', 'High'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "005ab20c-a7ba-45bb-9048-28cb54a845ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>SMA_5</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>RSI_14</th>\n",
       "      <th>MACD</th>\n",
       "      <th>MACD_Signal</th>\n",
       "      <th>Bollinger_b</th>\n",
       "      <th>ATR_14</th>\n",
       "      <th>Momentum_10</th>\n",
       "      <th>Relative_price</th>\n",
       "      <th>Norm_Volatily</th>\n",
       "      <th>Scal_RSI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999-07-26</td>\n",
       "      <td>84.923210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999-07-27</td>\n",
       "      <td>85.632195</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.056557</td>\n",
       "      <td>0.011311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999-07-28</td>\n",
       "      <td>85.907906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.122218</td>\n",
       "      <td>0.033493</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999-07-29</td>\n",
       "      <td>84.706596</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.076438</td>\n",
       "      <td>0.042082</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999-07-30</td>\n",
       "      <td>83.662720</td>\n",
       "      <td>84.966525</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.043573</td>\n",
       "      <td>0.024951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Close      SMA_5  SMA_20  SMA_50  RSI_14      MACD  \\\n",
       "0  1999-07-26  84.923210        NaN     NaN     NaN     NaN  0.000000   \n",
       "1  1999-07-27  85.632195        NaN     NaN     NaN     NaN  0.056557   \n",
       "2  1999-07-28  85.907906        NaN     NaN     NaN     NaN  0.122218   \n",
       "3  1999-07-29  84.706596        NaN     NaN     NaN     NaN  0.076438   \n",
       "4  1999-07-30  83.662720  84.966525     NaN     NaN     NaN -0.043573   \n",
       "\n",
       "   MACD_Signal  Bollinger_b  ATR_14  Momentum_10  Relative_price  \\\n",
       "0     0.000000          NaN     NaN          NaN             NaN   \n",
       "1     0.011311          NaN     NaN          NaN             NaN   \n",
       "2     0.033493          NaN     NaN          NaN             NaN   \n",
       "3     0.042082          NaN     NaN          NaN             NaN   \n",
       "4     0.024951          NaN     NaN          NaN             NaN   \n",
       "\n",
       "   Norm_Volatily  Scal_RSI  \n",
       "0            NaN       NaN  \n",
       "1            NaN       NaN  \n",
       "2            NaN       NaN  \n",
       "3            NaN       NaN  \n",
       "4            NaN       NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_SPY.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05fdff87-8283-459e-97a2-f0671d6ea5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>SMA_5</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>RSI_14</th>\n",
       "      <th>MACD</th>\n",
       "      <th>MACD_Signal</th>\n",
       "      <th>Bollinger_b</th>\n",
       "      <th>ATR_14</th>\n",
       "      <th>Momentum_10</th>\n",
       "      <th>Relative_price</th>\n",
       "      <th>Norm_Volatily</th>\n",
       "      <th>Scal_RSI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>92.142548</td>\n",
       "      <td>92.746373</td>\n",
       "      <td>90.878498</td>\n",
       "      <td>88.689529</td>\n",
       "      <td>64.905610</td>\n",
       "      <td>1.209557</td>\n",
       "      <td>1.134688</td>\n",
       "      <td>0.706253</td>\n",
       "      <td>1.204466</td>\n",
       "      <td>1.742256</td>\n",
       "      <td>1.013909</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.649056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>88.539215</td>\n",
       "      <td>91.930681</td>\n",
       "      <td>90.793552</td>\n",
       "      <td>88.815921</td>\n",
       "      <td>48.025427</td>\n",
       "      <td>0.852350</td>\n",
       "      <td>1.078220</td>\n",
       "      <td>0.150906</td>\n",
       "      <td>1.382858</td>\n",
       "      <td>-1.207680</td>\n",
       "      <td>0.975171</td>\n",
       "      <td>0.015619</td>\n",
       "      <td>0.480254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>88.697601</td>\n",
       "      <td>91.067470</td>\n",
       "      <td>90.753062</td>\n",
       "      <td>88.953776</td>\n",
       "      <td>46.416858</td>\n",
       "      <td>0.575409</td>\n",
       "      <td>0.977658</td>\n",
       "      <td>0.190058</td>\n",
       "      <td>1.478556</td>\n",
       "      <td>-2.415451</td>\n",
       "      <td>0.977351</td>\n",
       "      <td>0.016670</td>\n",
       "      <td>0.464169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2000-01-06</td>\n",
       "      <td>87.272079</td>\n",
       "      <td>89.940936</td>\n",
       "      <td>90.669935</td>\n",
       "      <td>89.083660</td>\n",
       "      <td>38.638978</td>\n",
       "      <td>0.238158</td>\n",
       "      <td>0.829758</td>\n",
       "      <td>0.025521</td>\n",
       "      <td>1.580544</td>\n",
       "      <td>-4.078476</td>\n",
       "      <td>0.962525</td>\n",
       "      <td>0.018111</td>\n",
       "      <td>0.386390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>2000-01-07</td>\n",
       "      <td>92.340523</td>\n",
       "      <td>89.798393</td>\n",
       "      <td>90.818503</td>\n",
       "      <td>89.285683</td>\n",
       "      <td>56.171171</td>\n",
       "      <td>0.375536</td>\n",
       "      <td>0.738913</td>\n",
       "      <td>0.711398</td>\n",
       "      <td>1.873088</td>\n",
       "      <td>-0.465248</td>\n",
       "      <td>1.016759</td>\n",
       "      <td>0.020285</td>\n",
       "      <td>0.561712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date      Close      SMA_5     SMA_20     SMA_50     RSI_14  \\\n",
       "112  2000-01-03  92.142548  92.746373  90.878498  88.689529  64.905610   \n",
       "113  2000-01-04  88.539215  91.930681  90.793552  88.815921  48.025427   \n",
       "114  2000-01-05  88.697601  91.067470  90.753062  88.953776  46.416858   \n",
       "115  2000-01-06  87.272079  89.940936  90.669935  89.083660  38.638978   \n",
       "116  2000-01-07  92.340523  89.798393  90.818503  89.285683  56.171171   \n",
       "\n",
       "         MACD  MACD_Signal  Bollinger_b    ATR_14  Momentum_10  \\\n",
       "112  1.209557     1.134688     0.706253  1.204466     1.742256   \n",
       "113  0.852350     1.078220     0.150906  1.382858    -1.207680   \n",
       "114  0.575409     0.977658     0.190058  1.478556    -2.415451   \n",
       "115  0.238158     0.829758     0.025521  1.580544    -4.078476   \n",
       "116  0.375536     0.738913     0.711398  1.873088    -0.465248   \n",
       "\n",
       "     Relative_price  Norm_Volatily  Scal_RSI  \n",
       "112        1.013909       0.013072  0.649056  \n",
       "113        0.975171       0.015619  0.480254  \n",
       "114        0.977351       0.016670  0.464169  \n",
       "115        0.962525       0.018111  0.386390  \n",
       "116        1.016759       0.020285  0.561712  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_SPY = inputs_SPY[inputs_SPY['Date'] >= '2000-01-01']\n",
    "inputs_SPY = inputs_SPY[inputs_SPY['Date'] <= '2025-01-01']\n",
    "inputs_SPY.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141ecf40-8d1e-4bbc-9a92-299974628580",
   "metadata": {},
   "source": [
    "### Creating the architecture of the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e52498f8-22a1-4c72-b27a-e0edd4aa5eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a17a411-b453-44dc-97ce-080b0b53572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim=17, output_dim=8):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, output_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))  # [batch_size, 256]\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))  # [batch_size, 128]\n",
    "        x = self.dropout(F.relu(self.fc3(x)))  # [batch_size, 64]\n",
    "        x = self.fc4(x)  # ¡Esta línea faltaba! [batch_size, 8]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee236015-6108-4e6d-bab1-29672fb81c76",
   "metadata": {},
   "source": [
    "### Create Custom Trading Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "680545f1-2c84-4c7f-80af-0da95ad922ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom Trading Environment for a single stock\n",
    "class TradingEnv:\n",
    "    \"\"\"\n",
    "    Initialize the trading environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, initial_cash=10_000):\n",
    "        # Load and sort data\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.max_steps = len(self.df) - 2  # total steps (days)\n",
    "\n",
    "        # Initial portfolio settings\n",
    "        self.initial_cash = initial_cash\n",
    "        self.cash = initial_cash\n",
    "        self.shares_held = 0.0\n",
    "        self.current_step = 0\n",
    "        self.prev_portfolio_value = initial_cash\n",
    "        self.returns_history = []\n",
    "\n",
    "        # Track total reward and history\n",
    "        self.total_reward = 0\n",
    "        self.history = []\n",
    "\n",
    "        # Action mapping\n",
    "        self.action_mapping = {\n",
    "            0: 0.05,  # buy small\n",
    "            1: 0.10,  # buy small - medium\n",
    "            2: 0.20,  # buy large - medium\n",
    "            3: 0.35,  # buy large\n",
    "            4: 0.10,  # sell small\n",
    "            5: 0.25,  # sell small - medium\n",
    "            6: 0.50,  # sell large - medium\n",
    "            7: 1.00   # sell all\n",
    "        }\n",
    "\n",
    "        # Define input features\n",
    "        self.feature_cols = [\n",
    "            'Close',\n",
    "            'SMA_5', 'SMA_20', 'SMA_50',\n",
    "            'RSI_14',\n",
    "            'MACD', 'MACD_Signal',\n",
    "            'Bollinger_b',\n",
    "            'ATR_14',\n",
    "            'Momentum_10',\n",
    "            'Relative_price', 'Norm_Volatily', 'Scal_RSI'\n",
    "        ]\n",
    "\n",
    "    \"\"\"\n",
    "    Reset the environment to the initial state.\n",
    "    \"\"\"\n",
    "    def reset(self):\n",
    "        self.cash = self.initial_cash          # Reset available capital\n",
    "        self.shares_held = 0.0                 # No shares at start\n",
    "        self.current_step = 0                  # Start at the beginning of the dataset\n",
    "        self.prev_portfolio_value = self.initial_cash  # Track portfolio for reward calc\n",
    "        self.total_reward = 0.0                # Reset reward tracker\n",
    "        self.history = []  \n",
    "        self.returns_history = []# Clear history\n",
    "    \n",
    "        return self._get_state()\n",
    "\n",
    "    \"\"\"\n",
    "    Construct the current state vector.\n",
    "    \"\"\"\n",
    "    def _get_state(self):\n",
    "        row = self.df.loc[self.current_step]\n",
    "    \n",
    "        # Extract the market features\n",
    "        features = []\n",
    "        for col in self.feature_cols:\n",
    "            features.append(row[col])\n",
    "    \n",
    "        # Append portfolio state\n",
    "        current_price = self.df.loc[self.current_step, 'Close']\n",
    "        features.append(self.shares_held * current_price / self.cash)  # ratio position/cash (ADD)\n",
    "        features.append(self.shares_held * current_price / self.initial_cash)  # % invest portfolio (ADD)\n",
    "        features.append(self.cash)\n",
    "        features.append(self.shares_held)\n",
    "    \n",
    "        return np.array(features, dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = 0.0\n",
    "        invalid_action_penalty = -.01\n",
    "    \n",
    "        current_price = self.df.loc[self.current_step, 'Close']\n",
    "        action_type = self.action_mapping[action]\n",
    "\n",
    "        #Make Step\n",
    "        if 0 <= action <= 3:  # buy\n",
    "            percent = self.action_mapping[action]\n",
    "            if self.cash >= current_price * action_type:\n",
    "                self._buy(percent)\n",
    "        elif 4 <= action <= 7:  # sell\n",
    "            percent = self.action_mapping[action]\n",
    "            if self.shares_held > 0:\n",
    "                self._sell(percent)\n",
    "\n",
    "        #Get portfolio Value\n",
    "        portfolio_value = self._get_portfolio_value()\n",
    "        \n",
    "        #Calculate Daily Return \n",
    "        daily_return = (portfolio_value - self.prev_portfolio_value) / (self.prev_portfolio_value + 1e-6)\n",
    "        self.returns_history.append(daily_return)\n",
    "\n",
    "        if len(self.returns_history) > 100:\n",
    "            self.returns_history.pop(0)\n",
    "\n",
    "        #Update peak_portfolio\n",
    "        #self.peak_portfolio = max(self.peak_portfolio, portfolio_value)\n",
    "\n",
    "        # Calculate reward \n",
    "        reward += self.calculate_reward(action)\n",
    "        self.total_reward += reward\n",
    "\n",
    "        # Advance to next time step\n",
    "        self.current_step += 1\n",
    "        \n",
    "        #Get new Price of stock\n",
    "        self.current_price = self.df.loc[self.current_step, 'Close']\n",
    "\n",
    "        # Update prev_portfolio value\n",
    "        self.prev_portfolio_value = portfolio_value\n",
    "    \n",
    "        # Log history for debugging/analysis\n",
    "        self.history.append({\n",
    "            'step': self.current_step,\n",
    "            'cash': self.cash,\n",
    "            'shares_held': self.shares_held,\n",
    "            'portfolio_value': portfolio_value,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "        })\n",
    "\n",
    "        done = self.current_step >= self.max_steps - 1\n",
    "        if done:\n",
    "\n",
    "            next_state = self._get_state()\n",
    "            return next_state, reward, done, {}\n",
    "    \n",
    "\n",
    "        next_state = self._get_state()\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "\n",
    "    def calculate_reward(self, action):\n",
    "        # Inicializar componentes\n",
    "        rewards = {\n",
    "            'pnl': 0.0,                  # Profit & Loss básico\n",
    "            'risk_adjusted': 0.0,        # Retorno ajustado por riesgo\n",
    "            'invalid_action': 0.0,       # Penalización por acción inválida\n",
    "            'position_management': 0.0,  # Manejo de posición óptimo\n",
    "            'trend_alignment': 0.0      # Alineación con tendencia\n",
    "        }\n",
    "        \n",
    "        # 1. Componente PnL (base)\n",
    "        current_value = self._get_portfolio_value()\n",
    "        pnl = (current_value - self.prev_portfolio_value) / (self.prev_portfolio_value + 1e-6)\n",
    "        rewards['pnl'] = np.clip(pnl * 5, -2.0, 2.0)  # Escalado y limitado\n",
    "        \n",
    "        # 2. Componente Ajustado por Riesgo (Sharpe-like)\n",
    "        returns_window = np.array(self.returns_history[-20:])  # Últimos 20 retornos\n",
    "        if len(returns_window) > 5:\n",
    "            volatility = np.std(returns_window) + 1e-6\n",
    "            risk_free = 0.0002  # Tasa libre de riesgo diaria aprox.\n",
    "            sharpe_like = (np.mean(returns_window) - risk_free) / volatility\n",
    "            rewards['risk_adjusted'] = np.clip(sharpe_like * 2, -1.5, 1.5)\n",
    "        \n",
    "        # 3. Penalización por Acción Inválida (¡Nueva recomendación!)\n",
    "        invalid_penalty = self._get_invalid_action_penalty(action)\n",
    "        rewards['invalid_action'] = invalid_penalty\n",
    "        \n",
    "        # 4. Gestión de Posición\n",
    "        rewards['position_management'] = self._calculate_position_score()\n",
    "        \n",
    "        # 5. Alineación con Tendencia\n",
    "        rewards['trend_alignment'] = self._calculate_trend_alignment()\n",
    "        \n",
    "        # Ponderación final\n",
    "        weights = {\n",
    "            'pnl': 0.4,\n",
    "            'risk_adjusted': 0.3,\n",
    "            'invalid_action': 0.15,\n",
    "            'position_management': 0.1,\n",
    "            'trend_alignment': 0.05\n",
    "        }\n",
    "        \n",
    "        total_reward = sum(rewards[component] * weights[component] for component in rewards)\n",
    "        return np.clip(total_reward, -3.0, 3.0)  # Limitar para estabilidad\n",
    "\n",
    "\n",
    "    def _get_invalid_action_penalty(self, action):\n",
    "        current_price = self.df.loc[self.current_step, 'Close']\n",
    "        action_type = self.action_mapping[action]\n",
    "        \n",
    "        # Penalización base escalonada\n",
    "        if (0 <= action <= 3) and (self.cash < current_price * action_type):\n",
    "            # Intento de compra sin fondos\n",
    "            base_penalty = -0.5\n",
    "            \n",
    "            # Penalización adicional proporcional a lo \"ambicioso\" de la acción\n",
    "            ambition_penalty = -0.3 * action_type  # Más fuerte para acciones mayores\n",
    "            \n",
    "            return base_penalty + ambition_penalty\n",
    "        \n",
    "        elif (4 <= action <= 7) and (self.shares_held <= 0):\n",
    "            # Intento de venta sin posición\n",
    "            base_penalty = -0.4\n",
    "            \n",
    "            # Penalización por tamaño de orden de venta\n",
    "            size_penalty = -0.2 * abs(action_type)\n",
    "            \n",
    "            return base_penalty + size_penalty\n",
    "        \n",
    "        return 0.0  # Acción válida\n",
    "\n",
    "    def _calculate_position_score(self):\n",
    "        \"\"\"Evalúa qué tan óptima es la posición actual\"\"\"\n",
    "        current_price = self.df.loc[self.current_step, 'Close']\n",
    "        position_ratio = (self.shares_held * current_price) / self._get_portfolio_value()\n",
    "        \n",
    "        # Ideal: 30-70% invertido (evita estar totalmente en cash o totalmente invertido)\n",
    "        optimal_min = 0.3\n",
    "        optimal_max = 0.7\n",
    "        \n",
    "        if position_ratio < optimal_min:\n",
    "            return -0.5 * (optimal_min - position_ratio)  # Penaliza estar bajo-invertido\n",
    "        elif position_ratio > optimal_max:\n",
    "            return -0.8 * (position_ratio - optimal_max)  # Penaliza más el sobre-invertir\n",
    "        else:\n",
    "            return 0.3  # Recompensa por estar en rango óptimo\n",
    "\n",
    "    def _calculate_trend_alignment(self):\n",
    "        \"\"\"Recompensa alinear acciones con tendencia del mercado\"\"\"\n",
    "        price_vs_sma = self.df.loc[self.current_step, 'Close'] / self.df.loc[self.current_step, 'SMA_20']\n",
    "        rsi = self.df.loc[self.current_step, 'RSI_14']\n",
    "        \n",
    "        # Lógica de tendencia\n",
    "        if price_vs_sma > 1.02 and rsi < 70:  # Tendencia alcista saludable\n",
    "            return 0.4 if self.shares_held > 0 else -0.3\n",
    "        elif price_vs_sma < 0.98 and rsi > 30:  # Tendencia bajista\n",
    "            return 0.3 if self.shares_held == 0 else -0.4\n",
    "        else:\n",
    "            return 0.1 if abs(self.shares_held) < 0.1 else 0.0  # Mercado lateral\n",
    "\n",
    "    def _get_portfolio_value(self):\n",
    "        current_price = self.df.loc[self.current_step, 'Close']\n",
    "        return self.cash + (self.shares_held * current_price)\n",
    "\n",
    "    \"\"\"\n",
    "    Execute a buy order using a percentage of available cash.\n",
    "    \"\"\"\n",
    "    def _buy(self, percent):\n",
    "        current_price = self.df.loc[self.current_step, 'Close']\n",
    "        \n",
    "        # Capital to use for this transaction\n",
    "        amount_to_spend = self.cash * percent\n",
    "    \n",
    "        # Number of whole shares we can buy\n",
    "        shares_to_buy = int(amount_to_spend // current_price)\n",
    "    \n",
    "        if shares_to_buy > 0:\n",
    "            self.cash -= shares_to_buy * current_price\n",
    "            self.shares_held += shares_to_buy\n",
    "\n",
    "    \"\"\"\n",
    "    Execute a sell order using a percentage of held shares.\n",
    "    \"\"\"\n",
    "    def _sell(self, percent):\n",
    "        current_price = self.df.loc[self.current_step, 'Close']\n",
    "    \n",
    "        # Determine how many shares to sell\n",
    "        shares_to_sell = int(self.shares_held * percent)\n",
    "    \n",
    "        if shares_to_sell > 0:\n",
    "            self.cash += shares_to_sell * current_price\n",
    "            self.shares_held -= shares_to_sell\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56645e7-36c6-4621-bed1-9f2667b78cc7",
   "metadata": {},
   "source": [
    "### Initializing the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a76feac-d703-46e5-b073-5f9d7fcf8576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate for optimizer (controls how fast the model learns)\n",
    "learning_rate = 3e-4 \n",
    "# Number of experiences used for each learning step\n",
    "minibatch_size = 64\n",
    "# Discount factor (γ): how much future rewards are valued vs. immediate rewards\n",
    "discount_factor = 0.9\n",
    "#discount_factor = 0.99\n",
    "# Size of the experience replay buffer\n",
    "replay_buffer_size = int(1e6)\n",
    "# Soft update rate (τ): how fast target network updates towards the main network\n",
    "interpolation_parameter = 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2703e1d-e20c-41e7-adf8-b68178ef0ab5",
   "metadata": {},
   "source": [
    "### Implementing Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8533dd13-a6c5-4d01-b14b-5439a27295a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, event):\n",
    "        self.memory.append(event)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        experiences = random.sample(self.memory, k=batch_size)\n",
    "    \n",
    "        states = torch.from_numpy(\n",
    "            np.vstack([e[0] for e in experiences if e is not None])\n",
    "        ).float().to(self.device)\n",
    "    \n",
    "        actions = torch.from_numpy(\n",
    "            np.vstack([e[1] for e in experiences if e is not None])\n",
    "        ).long().to(self.device)\n",
    "    \n",
    "        rewards = torch.from_numpy(\n",
    "            np.vstack([e[2] for e in experiences if e is not None])\n",
    "        ).float().to(self.device)\n",
    "    \n",
    "        next_states = torch.from_numpy(\n",
    "            np.vstack([e[3] for e in experiences if e is not None])\n",
    "        ).float().to(self.device)\n",
    "    \n",
    "        dones = torch.from_numpy(\n",
    "            np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)\n",
    "        ).float().to(self.device)\n",
    "    \n",
    "        return states, next_states, actions, rewards, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18adcbe-4a6a-4187-94b7-0cbd1e4dc4f5",
   "metadata": {},
   "source": [
    "### Create DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f59c70c-4f52-4b1b-9216-fc9a8a00624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.local_qnetwork = DQN(state_size, action_size).to(self.device)\n",
    "        self.target_qnetwork = DQN(state_size, action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr = learning_rate)\n",
    "        self.memory = ReplayMemory(replay_buffer_size)\n",
    "        self.t_step = 0\n",
    "        self.best_score = -np.inf\n",
    "        self.best_model_state = None\n",
    "        self.validation_interval = 100  # Cada cuántos episodios validar\n",
    "        self.patience = 5\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.push((state, action, reward, next_state, done))\n",
    "        self.t_step = (self.t_step + 1) % 4\n",
    "        if self.t_step == 0:\n",
    "          if len(self.memory.memory) > minibatch_size:\n",
    "            experiences = self.memory.sample(64)\n",
    "            self.learn(experiences, discount_factor)\n",
    "\n",
    "    def act(self, state, epsilon = 0.):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.local_qnetwork.eval()\n",
    "        with torch.no_grad():\n",
    "          action_values = self.local_qnetwork(state)\n",
    "        self.local_qnetwork.train()\n",
    "        if random.random() > epsilon:\n",
    "          return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "          return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, discount_factor):\n",
    "        states, next_states, actions, rewards, dones = experiences\n",
    "        next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        q_targets = rewards + discount_factor * next_q_targets * (1 - dones)\n",
    "        q_expected = self.local_qnetwork(states).gather(1, actions)\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.soft_update(self.local_qnetwork, self.target_qnetwork, interpolation_parameter)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, interpolation_parameter):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "          target_param.data.copy_(interpolation_parameter * local_param.data + (1.0 - interpolation_parameter) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d328a4c-9bac-4b14-9575-c0ca7bfe9938",
   "metadata": {},
   "source": [
    "### Initializing iteration of DQN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aabd36a4-b250-4566-b9a0-5223a0354231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1256\n",
      "Oldst Date:  2000-01-03\n",
      "Recent Date:  2004-12-31\n",
      "252\n",
      "Oldst Date:  2005-01-03\n",
      "Recent Date:  2005-12-30\n",
      "---------------------------------------\n",
      "1508\n",
      "Oldst Date:  2000-01-03\n",
      "Recent Date:  2005-12-30\n",
      "251\n",
      "Oldst Date:  2006-01-03\n",
      "Recent Date:  2006-12-29\n",
      "---------------------------------------\n",
      "1759\n",
      "Oldst Date:  2000-01-03\n",
      "Recent Date:  2006-12-29\n",
      "251\n",
      "Oldst Date:  2007-01-03\n",
      "Recent Date:  2007-12-31\n",
      "---------------------------------------\n",
      "2010\n",
      "Oldst Date:  2000-01-03\n",
      "Recent Date:  2007-12-31\n",
      "253\n",
      "Oldst Date:  2008-01-02\n",
      "Recent Date:  2008-12-31\n",
      "---------------------------------------\n",
      "2263\n",
      "Oldst Date:  2000-01-03\n",
      "Recent Date:  2008-12-31\n",
      "252\n",
      "Oldst Date:  2009-01-02\n",
      "Recent Date:  2009-12-31\n",
      "---------------------------------------\n",
      "2515\n",
      "Oldst Date:  2000-01-03\n",
      "Recent Date:  2009-12-31\n",
      "252\n",
      "Oldst Date:  2010-01-04\n",
      "Recent Date:  2010-12-31\n",
      "---------------------------------------\n",
      "2767\n",
      "Oldst Date:  2000-01-03\n",
      "Recent Date:  2010-12-31\n",
      "252\n",
      "Oldst Date:  2011-01-03\n",
      "Recent Date:  2011-12-30\n",
      "---------------------------------------\n",
      "3019\n",
      "Oldst Date:  2000-01-03\n",
      "Recent Date:  2011-12-30\n",
      "250\n",
      "Oldst Date:  2012-01-03\n",
      "Recent Date:  2012-12-31\n",
      "---------------------------------------\n",
      "3269\n",
      "Oldst Date:  2000-01-03\n",
      "Recent Date:  2012-12-31\n",
      "252\n",
      "Oldst Date:  2013-01-02\n",
      "Recent Date:  2013-12-31\n",
      "---------------------------------------\n",
      "3521\n",
      "Oldst Date:  2000-01-03\n",
      "Recent Date:  2013-12-31\n",
      "252\n",
      "Oldst Date:  2014-01-02\n",
      "Recent Date:  2014-12-31\n",
      "---------------------------------------\n",
      "3773\n",
      "Oldst Date:  2000-01-03\n",
      "Recent Date:  2014-12-31\n",
      "252\n",
      "Oldst Date:  2015-01-02\n",
      "Recent Date:  2015-12-31\n",
      "---------------------------------------\n",
      "4025\n",
      "Oldst Date:  2000-01-03\n",
      "Recent Date:  2015-12-31\n",
      "252\n",
      "Oldst Date:  2016-01-04\n",
      "Recent Date:  2016-12-30\n",
      "---------------------------------------\n",
      "4277\n",
      "Oldst Date:  2000-01-03\n",
      "Recent Date:  2016-12-30\n",
      "251\n",
      "Oldst Date:  2017-01-03\n",
      "Recent Date:  2017-12-29\n",
      "---------------------------------------\n",
      "4528\n",
      "Oldst Date:  2000-01-03\n",
      "Recent Date:  2017-12-29\n",
      "251\n",
      "Oldst Date:  2018-01-02\n",
      "Recent Date:  2018-12-31\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "initial_train_year = 2000\n",
    "test_year = 2005\n",
    "\n",
    "while test_year <= 2018:\n",
    "    training_SPY = inputs_SPY.copy()\n",
    "    training_SPY = training_SPY[training_SPY['Date'] >= str(initial_train_year)+'-01-01']\n",
    "    training_SPY = training_SPY[training_SPY['Date'] <= str(test_year)+'-01-01']\n",
    "\n",
    "    testing_SPY = inputs_SPY.copy()\n",
    "    testing_SPY = testing_SPY[testing_SPY['Date'] >= str(test_year)+'-01-01']\n",
    "    testing_SPY = testing_SPY[testing_SPY['Date'] <= str(test_year + 1)+'-01-01']\n",
    "\n",
    "    agent = Agent(17, 8)\n",
    "    env = TradingEnv(training_SPY)\n",
    "\n",
    "    print(\"Training year \" + str(test_year))\n",
    "    number_episodes = 2000\n",
    "    maximum_number_timesteps_per_episode = 1000\n",
    "    epsilon_starting_value  = 1.0\n",
    "    epsilon_ending_value  = 0.01\n",
    "    #epsilon_ending_value  = 0.1 \n",
    "    epsilon_decay_value  = 0.995\n",
    "    #epsilon_decay_value  = 0.95\n",
    "    epsilon = epsilon_starting_value\n",
    "    scores_on_400_episodes = deque(maxlen = 400)\n",
    "    total_value_on_400_episodes = deque(maxlen = 400)\n",
    "    \n",
    "    for episode in range(1, number_episodes + 1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        #for t in range(maximum_number_timesteps_per_episode):\n",
    "        for t in range(env.max_steps):\n",
    "            action = agent.act(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "              break\n",
    "        scores_on_400_episodes.append(score)\n",
    "        total_value_on_400_episodes.append(env.prev_portfolio_value)\n",
    "        epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)\n",
    "\n",
    "        if episode % 400 == 0:\n",
    "            print('\\rEpisode {}  Average Reward: {:.2f} Average Total Value: {:.2f}'.format(episode, np.mean(scores_on_400_episodes), np.mean(total_value_on_400_episodes)))\n",
    "\n",
    "    filename = 'Models/dqn_Experiment_4_'+str(test_year)+'.pth'\n",
    "    torch.save(agent.local_qnetwork.state_dict(), filename)\n",
    "    test_year += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf263f-7df9-4409-b650-e245144dae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_episodes = 2000\n",
    "maximum_number_timesteps_per_episode = 1000\n",
    "epsilon_starting_value  = 1.0\n",
    "epsilon_ending_value  = 0.01\n",
    "#epsilon_ending_value  = 0.1 \n",
    "epsilon_decay_value  = 0.995\n",
    "#epsilon_decay_value  = 0.95\n",
    "epsilon = epsilon_starting_value\n",
    "scores_on_100_episodes = deque(maxlen = 100)\n",
    "total_value_on_100_episodes = deque(maxlen = 100)\n",
    "\n",
    "for episode in range(1, number_episodes + 1):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    #for t in range(maximum_number_timesteps_per_episode):\n",
    "    for t in range(env.max_steps):\n",
    "        action = agent.act(state, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done:\n",
    "          break\n",
    "    scores_on_100_episodes.append(score)\n",
    "    total_value_on_100_episodes.append(env.prev_portfolio_value)\n",
    "    epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)\n",
    "    \n",
    "    print('\\rEpisode {}  Score: {:.2f} Initial Catch: {:.2f}  Total Catch {:.2f}, Total Holds:{}, Price stock: {:.2f}, Total Value: {:.2f}'.format(episode, score, env.initial_cash, env.cash, env.shares_held, env.current_price, env.prev_portfolio_value), end = \"\")\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        print('')\n",
    "        print('\\rEpisode {}  Average Reward: {:.2f} Average Total Value: {:.2f}'.format(episode, np.mean(scores_on_100_episodes), np.mean(total_value_on_100_episodes)))\n",
    "\n",
    "torch.save(agent.local_qnetwork.state_dict(), 'Models/dqn_trading_Experiment_3.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_7_thesis",
   "language": "python",
   "name": "py_7_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
